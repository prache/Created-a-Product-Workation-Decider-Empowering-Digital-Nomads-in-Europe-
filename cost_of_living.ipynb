{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section 1.1.1 <a class=\"anchor\" id=\"sub_section_1_1_1\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "* [Importing and cleaning the Data](#Importdata)\n",
    "    * [Global living costs](#Globallivingcosts) \n",
    "        * [Create a Data frame](#Createadataframe)\n",
    "        * [Handling missing values](#Handlingmissingvalues)\n",
    "        * [Data frame only for European 47 countries](#Dataframe_only_for_European_47_countries)\n",
    "        * [Tuple to list conversion](#Tupletolist)\n",
    "        * [Drop the columns](#dropcolumns)\n",
    "        * [Extract data frame to excel](#dftoexcel)\n",
    "        * [Filter the european countries](#countryfilter)\n",
    "    * [Global life expectancy](#globallifeexpectancy)\n",
    "    * [Global health expenditure](#globalhealthexpenditure)\n",
    "    * [GDP](#gdp)\n",
    "    * [Internet speed](#internetspeed)\n",
    "    * [Crime rate](#crimerate)\n",
    "    * [Happy planet index](#happyplanetindex)\n",
    "    * [Importing the weather data from database dBeaver](#weather)\n",
    "\n",
    "* [EDA](#eda)\n",
    "    * [Basket creation](#basket)\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data <a class=\"anchor\" id=\"Importdata\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global living costs <a class=\"anchor\" id=\"Globallivingcosts\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Data frame <a class=\"anchor\" id=\"Createadataframe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "# Set column names to 'carrier' and 'name' # It made th ecolumns\n",
    "global_living_costs_columns = ['city' , 'country', 'inexpensive_meal', 'mid_range_meal_for_two', 'mac_meal', 'domestic_beer_0.5', 'imported_beer_0.33', 'cappuccino_restaurants', \n",
    "                        'coke_0.33_restaurants', 'water_0.33_restaurants', 'milk_1_liter', 'loaf_fresh_white_bread_500g', 'white_rice_1kg', '12_eggs', 'local_cheese_1kg', \n",
    "                        'chicken_fillet_1kg', 'beef_1kg', 'apples_1kg', 'banana_1kg', 'oranges_1kg', 'tomato_1kg', 'potato_1kg', 'onion_1kg', 'lettuce_1_head', 'water_1.5ltr_market', \n",
    "                        'bottle_of_wine_market','domestic_beer_0.5ltr_market', 'imported_beer_0.33ltr_market', 'cigarettes_20packs_marlboro', 'one_way_ticket_local_transport', \n",
    "                        'monthly_pass', 'taxi_start', 'taxi_1km', 'taxi_1hr_waiting', 'gasoline_1ltr', 'car1_vw_golf', 'car2_toyota_corolla', 'apartment_extra_bills_85sqm', \n",
    "                        '1min__prepaid_call', 'internet', 'fitness_club_monthly', 'tennis_court_rent_1hr_weekend', 'cinema_1seat', 'kindergarten_full_day_private_monthly', \n",
    "                        'primary_school_yearly', 'one_jeans', 'one_summer_dress', 'nike_running_shoes_pair', 'leather_business_shoes_men', 'apartment_1_bed_city_center', \n",
    "                        'apartment_1_bed_outside_city_center', 'apartment_3_bed_city_center','apartment_3_bed_outside_city_center', 'price_per_sqm_buy_apartment_city_center',\n",
    "                        'price_per_sqm_buy_apartment_outside_city_center', 'avg_monthly_salary', 'mortgage_interest_rate_percent', 'data_quality']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_living_costs = pd.read_csv('cost-of-living_v2.csv', delimiter=',')\n",
    "global_living_costs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_living_costs = global_living_costs.rename(columns= {\"x1\":'inexpensive_meal', \"x2\":'mid_range_meal_for_two', \"x3\":'mac_meal', \"x4\":'domestic_beer_0.5', \"x5\":'imported_beer_0.33', \"x6\":'cappuccino_restaurants', \n",
    "                        \"x7\":'coke_0.33_restaurants', \"x8\":'water_0.33_restaurants', \"x9\":'milk_1_liter', \"x10\":'loaf_fresh_white_bread_500g', \"x11\":'white_rice_1kg', \"x12\":'12_eggs', \n",
    "                        \"x13\":'local_cheese_1kg', \n",
    "                        \"x14\":'chicken_fillet_1kg', \"x15\":'beef_1kg', \"x16\":'apples_1kg', \"x17\":'banana_1kg', \"x18\":'oranges_1kg', \"x19\":'tomato_1kg', \"x20\":'potato_1kg', \n",
    "                        \"x21\":'onion_1kg', \"x22\":'lettuce_1_head', \"x23\":'water_1.5ltr_market', \n",
    "                        \"x24\":'bottle_of_wine_market', \"x25\":'domestic_beer_0.5ltr_market', \"x26\":'imported_beer_0.33ltr_market', \"x27\":'cigarettes_20packs_marlboro', \n",
    "                        \"x28\":'one_way_ticket_local_transport', \n",
    "                        \"x29\":'monthly_pass', \"x30\":'taxi_start', \"x31\":'taxi_1km', \"x32\":'taxi_1hr_waiting', \"x33\":'gasoline_1ltr', \"x34\":'car1_vw_golf', \"x35\":'car2_toyota_corolla', \n",
    "                        \"x36\":'apartment_extra_bills_85sqm', \n",
    "                        \"x37\":'1min__prepaid_call', \"x38\":'internet', \"x39\":'fitness_club_monthly', \"x40\":'tennis_court_rent_1hr_weekend', \"x41\":'cinema_1seat', \"x42\":'kindergarten_full_day_private_monthly', \n",
    "                        \"x43\":'primary_school_yearly', \"x44\":'one_jeans', \"x45\":'one_summer_dress', \"x46\":'nike_running_shoes_pair', \"x47\":'leather_business_shoes_men', \"x48\":'apartment_1_bed_city_center', \n",
    "                        \"x49\":'apartment_1_bed_outside_city_center', \"x50\":'apartment_3_bed_city_center', \"x51\":'apartment_3_bed_outside_city_center', \"x52\":'price_per_sqm_buy_apartment_city_center',\n",
    "                        \"x53\":'price_per_sqm_buy_apartment_outside_city_center', \"x54\":'avg_monthly_salary', \"x55\":'mortgage_interest_rate_percent'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling missing values <a class=\"anchor\" id=\"Handling missing values\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_living_costs.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data frame only for European 47 countries <a class=\"anchor\" id=\"Dataframe_only_for_European_47_countries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_european_countries = ('Albania',\n",
    "'Andorra',\n",
    "'Belgium',\n",
    "'Bosnia and Herzegovina',\n",
    "'Bulgaria',\n",
    "'Denmark',\n",
    "'Germany',\n",
    "'Estonia',\n",
    "'Finland',\n",
    "'France',\n",
    "'Greece',\n",
    "'Ireland',\n",
    "'Iceland',\n",
    "'Italy',\n",
    "'Kazakhstan',\n",
    "'Kosovo',\n",
    "'Croatia',\n",
    "'Latvia',\n",
    "'Liechtenstein',\n",
    "'Lithuania',\n",
    "'Luxembourg',\n",
    "'Malta',\n",
    "'Moldova',\n",
    "'Monaco',\n",
    "'Montenegro',\n",
    "'Netherlands',\n",
    "'North Macedonia',\n",
    "'Norway',\n",
    "'Austria',\n",
    "'Poland',\n",
    "'Portugal',\n",
    "'Romania',\n",
    "'Russia',\n",
    "'San Marino',\n",
    "'Sweden',\n",
    "'Switzerland',\n",
    "'Serbia',\n",
    "'Slovakia',\n",
    "'Slovenia',\n",
    "'Spain',\n",
    "'Czech Republic',\n",
    "'Turkey',\n",
    "'Ukraine',\n",
    "'Hungary',\n",
    "'Vatican',\n",
    "'United Kingdom',\n",
    "'Belarus')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuple to list conversion <a class=\"anchor\" id=\"tupletolist\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_european_countries = list(list_european_countries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter the European countries <a class=\"anchor\" id=\"countryfilter\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_living_costs = global_living_costs.loc[global_living_costs[\"country\"].isin(list_european_countries)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop the redundant columns <a class=\"anchor\" id=\"dropcolumns\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_living_costs = european_living_costs.drop(['monthly_pass', 'tennis_court_rent_1hr_weekend', 'primary_school_yearly', 'apartment_1_bed_city_center', \n",
    "                                                                'apartment_1_bed_outside_city_center',\n",
    "                                                                'apartment_3_bed_city_center', 'apartment_3_bed_outside_city_center', 'price_per_sqm_buy_apartment_city_center', \n",
    "                                                                'price_per_sqm_buy_apartment_outside_city_center', 'mortgage_interest_rate_percent',\n",
    "                                                                'data_quality'], axis = 1) #inplace= true, may be needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final check on the dataframe, we have 1839 entries on 1503 rows and 47 columns with 47 countries\n",
    "final_european_living_costs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these_cities = [\"Armavir\", \"Novomoskovsk\", \"Prokuplje\", \"Citta di Castello\", \"Fidenza\", \"Iglesias\", \"Novy Jicin\", \"Ivdel\", \"Lipari\", \"Voerendaal\", \"Bertinoro\", \"Serris\", \n",
    "                    \"Castrolibero\", \"Merida\", \"Bethune\", \"Crotone\", \"Saint-Louis\", \"Montceau-les-Mines\", \"Lanskroun\", \"Biasca\", \"Cortina d'Ampezzo\", \"Falconara Marittima\", \n",
    "                    \"Buochs\", \"Champigny-sur-Marne\", \"Newcastle\", \"Zlatoust\", \"Oranienburg\", \"Chita\", \"Kyzyl\", \"Maisons-Alfort\", \"Bilhorod-Dnistrovskyi\", \"Cholet\", \"Ainring\", \"Bilovec\", \n",
    "                    \"Puteaux\", \"Meaux\", \"Rodez\", \"Beziers\", \"Arsenyev\", \"Fastiv\", \"Capannori\", \"San Dona di Piave\", \"Zakopane\", \"Komlo\", \"Santa Margherita Ligure\", \"Lambersart\", \"Casandrino\",\n",
    "                     \"Godalming\", \"Geraardsbergen\", \"Bratsk\", \"Gravesend\", \"Cattolica\", \"Manfredonia\", \"Auxerre\", \"Sutton on Hull\", \"Kolomyia\", \"Bad Mergentheim\", \"Livadeia\", \"Podebrady\", \n",
    "                     \"Vocklabruck\", \"Alatyr\", \"Angarsk\", \"Rudnyy\", \"Montreuil\", \"Rayleigh\", \"Potenza\", \"Evry\", \"Montevrain\", \"Birsfelden\", \"Gummersbach\", \"Bucine\", \"Grays\", \"Cuprija\", \"Tuapse\", \n",
    "                     \"Niort\", \"Lousada\", \"Saint-Martin-de-Crau\", \"Bollnas\", \"Zweibrucken\", \"Segezha\", \"Komsomol'sk-na-Amure\", \"Abakan\", \"Kettering\", \"Chalon-sur-Saone\", \"Vigevano\", \"Bad Kreuznach\",\n",
    "                      \"Veldhoven\", \"Louvain-la-Neuve\", \"Friedrichsdorf\", \"Litvinov\", \"Annaberg-Buchholz\", \"Alcaudete\", \"Ciudad de Ceuta\", \"Gallarate\", \"Salgotarjan\", \"Leninsk-Kuznetskiy\", \"Pulawy\", \n",
    "                      \"Nadym\", \"Qapshaghay\", \"Cherbourg\", \"Rincon de la Victoria\", \"Harderwijk\", \"San Martino Buon Albergo\", \"Weymouth\", \"Neubiberg\", \"Gatchina\", \"Grosseto\", \"Sighetu Marmatiei\", \"Vittorio Veneto\", \"Montecatini Terme\", \"Lachen\", \"Odorheiu Secuiesc\", \"Kiskunfelegyhaza\", \"Kisvarda\", \"Myjava\", \"Aigle\", \"Sibenik\", \"Polva\", \"Wolfenbuttel\", \"Nysa\", \"Nizhnevartovsk\", \"Great Linford\", \"Fontaines-sur-Saone\", \"Ta' Xbiex\", \"Wesel\", \"Solnechnogorsk\", \"Geel\", \"Orekhovo-Borisovo Yuzhnoye\", \"Saint-Maur-des-Fosses\", \"Rho\", \"Poissy\", \"Alcala de Guadaira\", \"Canosa di Puglia\", \"Mnisek pod Brdy\", \"Hartlepool\", \"Kedainiai\", \"Sremski Karlovci\", \"Frydek-Mistek\", \"La Seyne-sur-Mer\", \"Cagnes-sur-Mer\", \"Douai\", \"Zebbug\", \"Civitavecchia\", \"Landsberg\", \"Koscierzyna\", \"Husum\", \"Elvas\", \"Lienz\", \"Vitry-sur-Seine\", \"Clichy\", \"Inowroclaw\", \"Chambery\", \"Kars\", \"Wloclawek\", \"Rudesheim am Rhein\", \"Thoiry\", \"Petropavl\", \"Courbevoic\", \"Spijkenisse\", \"Neuilly-sur-Seine\", \"Alexandria\", \"Neu-Ulm\", \"Slonim\", \"Samokov\", \"Mendrisio\", \"Seasalter\", \"Levanto\", \"Castellammare di Stabia\", \"Skofja Loka\", \"Jurbarkas\", \"Kocevje\", \"Sveti Nikole\", \"Kriva Palanka\", \"Raseiniai\", \"Saintes\", \"Gross-Umstadt\", \"Feltre\", \"Cesky Krumlov\", \"Antony\", \"Clamart\", \"Bastia\", \"Villefranche-sur-Saone\", \"Zagorje\", \"Crnomelj\", \"Esztergom\", \"Chios\", \"Bad Honnef am Rhein\", \"Ariano Irpino\", \"Gjovik\", \"Astrakhan\", \"Velikiy Novgorod\", \"Yevpatoriia\", \"Tamworth\", \"Ewell\", \"Cahul\", \"Nevsehir\", \"Pozzuoli\", \"Schio\", \"Crailsheim\", \"Maloyaroslavets\", \"Villacidro\", \"Manavgat\", \"Qyzylorda\", \"Cumbernauld\", \"Druskininkai\", \"Kirkwall\", \"Konin\", \"Affoltern am Albis\", \"Seferhisar\", \"Kerch\", \"Vrilissia\", \"Taraclia\", \"Gulbene\", \"Braine-l'Alleud\", \"Hamm\", \"Nefteyugansk\", \"Johvi\", \"Postojna\", \"Gelendzhik\", \"Kempten\", \"Antequera\", \"Dupnitsa\", \"Pfaffenhofen\", \"Wolfsberg\", \"Mytishchi\", \"Petropavlovsk-Kamchatskiy\", \"Serpukhov\", \"Nikopol\", \"Merthyr Tudful\", \"Macclesfield\", \"Barlad\", \"Saldus\", \"Talsi\", \"Madona\", \"Bedzin\", \"Chateauroux\", \"Ornskoldsvik\", \"Malacky\", \"Blagoveshchensk\", \"Sievierodonetsk\", \"Elbasan\", \"Hunedoara\", \"Smolyan\", \"Peshkopi\", \"Kavadarci\", \"Dunkerque\", \"Lisburn\", \"Bad Salzuflen\", \"Leonberg\", \"Marinha Grande\", \"Trebic\", \"Krotoszyn\", \"Stradella\", \"Treherbert\", \"Syktyvkar\", \"Hoofddorp\", \"Algeciras\", \"Murom\", \"Schwerin\", \"Lobnya\", \"Pazardzhik\", \"Lowestoft\", \"Vaslui\", \"Kirkcaldy\", \"Kajaani\", \"Plunge\", \"Escaldes-Engordany\", \"Steinkjer\", \"Sant Julia de Loria\", \"Zajecar\", \"Velikiye Luki\", \"Villingen-Schwenningen\", \"Penafiel\", \"Merignac\", \"Tournai\", \"Matera\", \"Offenburg\", \"Aldershot\", \"Herzogenrath\", \"Boleslawiec\", \"Ostuni\", \"Herzogenaurach\", \"Kilkis\", \"Meilen\", \"Ciechocinek\", \"Capriate San Gervasio\", \"Gorebridge\", \"Vladimir\", \"Krasnogorsk\", \"Dos Hermanas\", \"Besancon\", \"Bottrop\", \"Zhukovskiy\", \"Magadan\", \"Vidin\", \"Chichester\", \"Allschwil\", \"Ogre\", \"Wotton-under-Edge\", \"Diekirch\", \"Landshut\", \"Russelsheim\", \"Brasschaat\", \"Pinerolo\", \"Arlon\", \"Rambouillet\", \"Wangen im Allgau\", \"Cesenatico\", \"Csongrad\", \"Matosinhos\", \"Melitopol\", \"Nakhodka\", \"Novyy Urengoy\", \"Sesto San Giovanni\", \"Ajaccio\", \"Ponta Delgada\", \"Lytkarino\", \"Opfikon\", \"Acheres\", \"Yozgat\", \"Zejtun\", \"Rabat\", \"Wiltz\", \"Titel\", \"Leszno\", \"Tarnowskie Gory\", \"Viareggio\", \"Cerignola\", \"Kleve\", \"Sesimbra\", \"Borlange\", \"Bra\", \"Veendam\", \"Rovinj\", \"Taglio\", \"Drobak\"]\n",
    "final_european_living_costs = final_european_living_costs[~final_european_living_costs['city'].isin(drop_these_cities)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract data frame to excel <a class=\"anchor\" id=\"dftoexcel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_living_costs.to_excel(\"final_european_living_costs.xlsx\", sheet_name='living_costs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_living_costs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "import psycopg2\n",
    "# Import get_engine from sql_functions.py. You will need to restart your kernel and rerun at this point since we changed the module since we first imported it.\n",
    "from sql_functions import get_engine \n",
    "# create a variable called engine using the get_engine function\n",
    "engine = get_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_1 = 'final_european_living_costs'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_living_costs.to_sql(name=table_1, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_1} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_living_costs.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on 'Are we working more than before?' <a class=\"anchor\" id=\"workingmorethanbefore\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "global_working_spans = pd.read_csv('annual-working-hours-per-worker.csv', delimiter=',')\n",
    "global_working_spans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the column\n",
    "global_working_spans = global_working_spans.rename(columns={'Entity': \"country\", 'Year': \"year\", 'Average annual working hours per worker': \"avg working hrs per worker\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns\n",
    "global_working_spans = global_working_spans.drop(['Code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match and access column 'country' from both the data frames, we have 31 countries matched\n",
    "european_working_spans = global_working_spans.loc[global_working_spans[\"country\"].isin(list_european_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter years to 2021-22, however we have latest 2017 \n",
    "european_working_spans[\"year\"].max()\n",
    "final_european_working_spans = european_working_spans.loc[european_working_spans['year'].isin({2017})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on null values, good that we don't have any nulls in this data\n",
    "final_european_working_spans.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_working_spans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract df to excel\n",
    "final_european_working_spans.to_excel(\"european_working_spans.xlsx\", sheet_name=\"are we working more\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global life expectancy <a class=\"anchor\" id=\"globallifeexpectancy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "global_gle = pd.read_csv('global-life_expectancy.csv', delimiter=',')\n",
    "global_gle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_gle = global_gle.rename(columns= {\"Entity\":'country', \"Code\":'code', \"Year\":'year', \"Indicator:Life expectancy at birth (years) - Sex:Both sexes\":'life expectancy'})\n",
    "global_gle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_gle[\"year\"].max()\n",
    "global_gle = global_gle.loc[global_gle['year'].isin({2019})]\n",
    "global_gle.head()\n",
    "global_gle.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_gle = global_gle.loc[global_gle[\"country\"].isin(list_european_countries)]\n",
    "final_european_gle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_gle.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_gle.to_excel(excel_writer='european_life_expectancy.xlsx', sheet_name='final_european_gle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_2 = 'final_european_gle'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_gle.to_sql(name=table_2, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_2} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on 'Global health expenditure' <a class=\"anchor\" id=\"globalhealthexpenditure\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "global_health_expenditure = pd.read_csv('global-health-expenditure.csv', delimiter=',')\n",
    "global_health_expenditure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_health_expenditure = global_health_expenditure.rename(columns={\"Entity\":'country', \"Code\":'code', \"Year\":'year', \"Indicator:Current health expenditure (CHE) as percentage of gross domestic product (GDP) (%)\":'health_expenditure_percent_of_gdp'})\n",
    "global_health_expenditure.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_health_expenditure[\"year\"].max()\n",
    "global_health_expenditure = global_health_expenditure.loc[global_health_expenditure['year'].isin({2019})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_health_expenditure = global_health_expenditure.loc[global_health_expenditure['country'].isin(list_european_countries)] \n",
    "final_european_health_expenditure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_health_expenditure.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_health_expenditure.to_excel(excel_writer='european_health_expenditure.xlsx', sheet_name='health_expenditure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_3 = 'final_european_health_expenditure'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_health_expenditure.to_sql(name=table_3, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_3} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on Child Mortality <a class=\"anchor\" id=\"childmortality\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "child_mortality = pd.read_csv('child-mortality-igme.csv', delimiter=',')\n",
    "child_mortality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_mortality = child_mortality.rename(columns={\"Entity\":'country', \"Code\":'code', \"Year\":'year', \"Mortality rate, under-5 (per 1,000 live births)\":'mortality rate'})\n",
    "child_mortality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#child_mortality[\"year\"].max()\n",
    "child_mortality = child_mortality.loc[child_mortality['year'].isin({2020})]\n",
    "child_mortality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_child_mortality = child_mortality.loc[child_mortality['country'].isin(list_european_countries)]\n",
    "final_european_child_mortality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_child_mortality.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_child_mortality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_child_mortality.to_excel(excel_writer=\"child_mortality.xlsx\", sheet_name='child_mortality')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on GDP <a class=\"anchor\" id=\"gdp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "global_gdp = pd.read_csv('GDP.csv')\n",
    "global_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_gdp['2022'].isnull().sum()\n",
    "#global_gdp['2022'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_gdp = global_gdp.drop(['Subject Descriptor','Units','Scale', 'Country/Series-specific Notes', 'Estimates Start After', 'Unnamed: 14', '2020', '2021', '2023', '2024', '2025', '2026', '2027'], axis=1)\n",
    "global_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_gdp = global_gdp.rename(columns={\"Country\":'country'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_gdp = global_gdp.loc[global_gdp['country'].isin(list_european_countries)]\n",
    "#final_european_gdp.shape\n",
    "final_european_gdp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_gdp.to_excel(excel_writer=\"GDP.xlsx\", sheet_name='GDP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_4 = 'final_european_gdp'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_gdp.to_sql(name=table_4, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_4} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on Internet speed <a class=\"anchor\" id=\"internetspeed\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "global_internet_speed = pd.read_csv('Internet Speed 2022 2.csv')\n",
    "global_internet_speed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_internet_speed.columns = global_internet_speed.columns.str.lower()\n",
    "global_internet_speed = global_internet_speed.rename(columns={'broadband':'broadband_internet_speed', 'mobile':'mobile_internet_speed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_internet_speed['broadband_internet_speed'] = global_internet_speed.broadband_internet_speed.apply(lambda x: x/8) # divide megabits into megabytes by factor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_internet_speed = global_internet_speed.loc[global_internet_speed['country'].isin(list_european_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_internet_speed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_internet_speed.to_excel(excel_writer=\"internet_speed.xlsx\", sheet_name='internet speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_5 = 'final_european_internet_speed'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_internet_speed.to_sql(name=table_5, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_5} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on crime rate <a class=\"anchor\" id=\"crimerate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "global_crime_index = pd.read_csv('World Crime Index .csv')\n",
    "global_crime_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_crime_index = global_crime_index.drop(['Rank', 'Safety Index'], axis=1)\n",
    "global_crime_index = global_crime_index.rename(columns={\"City\":'city', \"Crime Index\":'crime index'})\n",
    "global_crime_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_columns = global_crime_index['city'].str.split(pat=', ', n=1, expand=True) \n",
    "global_crime_index[\"city\"] = splitted_columns[0]\n",
    "global_crime_index[\"country\"] = splitted_columns[1]\n",
    "global_crime_index = global_crime_index[[\"country\", \"city\", \"crime index\"]]\n",
    "global_crime_index.shape\n",
    "#splitted_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_crime_index = global_crime_index.loc[global_crime_index['country'].isin(list_european_countries)]\n",
    "final_european_crime_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_crime_index.to_excel(excel_writer=\"Crime index.xlsx\", sheet_name='crime index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_6 = 'final_european_crime_index'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_crime_index.to_sql(name=table_6, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_6} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy planet index <a class=\"anchor\" id=\"happyplanetindex\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv\n",
    "european_hpi_2019 = pd.read_csv('european_hpi_2019.csv')\n",
    "european_hpi_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_hpi_2019 = european_hpi_2019.drop(['year', 'life_expectancy', 'biocapacity_for_year_(g_ha)'], axis=1)\n",
    "european_hpi_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_hpi = european_hpi_2019.rename(columns={\"ladder_of_life_(0-10)\":'subjective_wellbeing_0-10',\"population_(thousands)\": 'population_thousands', \"footprint_(g_ha)\":'carbon_footprint_global_hectare', \"gdp_per_capita_(us_dollar)\":'gdp_per_capita_us_dollar'})\n",
    "final_european_hpi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_7 = 'final_european_hpi_2019'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        final_european_hpi_2019.to_sql(name=table_7, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_7} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the weather data from dBeaver <a class=\"anchor\" id=\"weather\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: password, host and username are in a different notebook, which will not be uploaded to github due to gitignore\n",
    "from sql_functions  import get_dataframe\n",
    "#define schema\n",
    "schema = 'bestteamever'\n",
    "\n",
    "# Import the needed tables from sql server and put it in dataframes\n",
    "\n",
    "final_weather_data = get_dataframe(f'select * from {schema}.weather_temp;')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_temp_pr = final_weather_data.rename(columns={\"1.1.2022\":'winter', \"1.4.2022\":'spring', \"1.7.2022\":'summer', \"1.10.2022\":'autumn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_temp_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "import psycopg2\n",
    "# Import get_engine from sql_functions.py. You will need to restart your kernel and rerun at this point since we changed the module since we first imported it.\n",
    "from sql_functions import get_engine \n",
    "# create a variable called engine using the get_engine function\n",
    "engine = get_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_8 = 'european_temp_pr'\n",
    "schema = 'bestteamever'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        european_temp_pr.to_sql(name=table_8, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_8} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA <a class=\"anchor\" id=\"eda\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How expensive is living in other countries compared to Germany?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Import get_dataframe function from the sql module\n",
    "from sql_functions  import get_dataframe\n",
    "import matplotlib # Imports matplotlib library\n",
    "import matplotlib.pyplot as plt # Imports pyplot\n",
    "import numpy as np # import numpy for b uilding arrays\n",
    "import seaborn as sns\n",
    "from sql_functions  import get_engine\n",
    "import sqlalchemy\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: password, host and username are in a different notebook, which will not be uploaded to github due to gitignore\n",
    "\n",
    "#define schema\n",
    "schema = 'bestteamever'\n",
    "\n",
    "# Import the needed tables from sql server and put it in dataframes\n",
    "\n",
    "final_european_living_costs = get_dataframe(f'select * from {schema}.final_european_living_costs;')\n",
    "\n",
    "final_european_internet_speed = get_dataframe(f'select * from {schema}.final_european_internet_speed;')\n",
    "\n",
    "final_european_hpi_2019 = get_dataframe(f'select * from {schema}.final_european_hpi_2019;')\n",
    "\n",
    "final_european_health_expenditure = get_dataframe(f'select * from {schema}.final_european_health_expenditure;')\n",
    "\n",
    "final_european_gle = get_dataframe(f'select * from {schema}.final_european_gle;')\n",
    "\n",
    "final_european_gdp = get_dataframe(f'select * from {schema}.final_european_gdp;')\n",
    "\n",
    "final_european_crime_index = get_dataframe(f'select * from {schema}.final_european_crime_index;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "final_european_living_costs[final_european_living_costs.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plot the mi amount of `inexpensive meal` for each `country` value. \n",
    "final_european_living_costs.groupby(\"country\").min()[\"inexpensive_meal\"].plot(kind=\"bar\",x=\"country\", y=\"inexpensive_meal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basket creation <a class=\"anchor\" id=\"basket\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that there are too many columns, we'll create a baskets and reduce the columns\n",
    "\n",
    "Meal outside: \n",
    "\n",
    "* mac_meal \n",
    "\n",
    "* meal_outside_for_two = mid_range_meal_for_two + water 0.33 + domestic_beer_0.5\n",
    "\n",
    "Cafe: \n",
    "\n",
    "* cappuccino_restaurant\n",
    "\n",
    "grocery store:\n",
    "\n",
    "* grocery_basket = milk_1_liter + loaf_fresh_white_bread_500g + white_rice_1kg + 12_eggs + local_cheese_1kg + chicken_fillet_1kg + apples_1kg + tomato_1kg + potato_1kg + onion_1kg + lettuce_1_head + water_1.5ltr_market + domestic_beer_0.5ltr_market\n",
    "\n",
    "transport: \n",
    "\n",
    "* gasoline_1ltr\n",
    "* taxi_1km\n",
    "\n",
    "work: \n",
    "\n",
    "* internet\n",
    "\n",
    "Entertainment: \n",
    "* cinema_1seat\n",
    "\n",
    "Education:\n",
    "* kindergarten_full_day_private_monthly\n",
    "\n",
    "shopping: \n",
    "\n",
    "* shopping = one_jeans + one_summer_dress + nike_running_shoes_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_for_baskets = final_european_living_costs[[\"city\",\n",
    "                                                 \"country\",\n",
    "                                                  \"mac_meal\",\n",
    "                                                  \"mid_range_meal_for_two\",\n",
    "                                                  \"water_0.33_restaurants\", \n",
    "                                                  \"domestic_beer_0.5\",\n",
    "                                                  \"cappuccino_restaurants\",\n",
    "                                                  \"milk_1_liter\",\n",
    "                                                  \"loaf_fresh_white_bread_500g\",\n",
    "                                                  \"white_rice_1kg\",\n",
    "                                                  \"12_eggs\",\n",
    "                                                  \"local_cheese_1kg\",\n",
    "                                                  \"chicken_fillet_1kg\",\n",
    "                                                  \"apples_1kg\",\n",
    "                                                  \"tomato_1kg\",\n",
    "                                                  \"potato_1kg\",\n",
    "                                                  \"onion_1kg\",\n",
    "                                                  \"lettuce_1_head\",\n",
    "                                                  \"water_1.5ltr_market\",\n",
    "                                                  \"domestic_beer_0.5ltr_market\",\n",
    "                                                  \"gasoline_1ltr\",\n",
    "                                                  \"taxi_1km\",\n",
    "                                                  \"internet\",\n",
    "                                                  \"cinema_1seat\",\n",
    "                                                  \"kindergarten_full_day_private_monthly\",\n",
    "                                                  \"one_jeans\",\n",
    "                                                  \"one_summer_dress\",\n",
    "                                                  \"nike_running_shoes_pair\"\n",
    "                                                     ]].copy()\n",
    "\n",
    "table_for_baskets[table_for_baskets.isna().any(axis=1)]                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the N/A from the column kindergarten, due to to much difference between the prices \n",
    "\n",
    "#table_for_baskets.dropna(subset = ['kindergarten_full_day_private_monthly'], how=\"all\", inplace= True)\n",
    "table_for_baskets.drop(table_for_baskets[table_for_baskets['kindergarten_full_day_private_monthly'] == \"NaN\"].index, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby country, build average for column and fill the null values with it \n",
    "\n",
    "for column in table_for_baskets.columns[2:]:\n",
    "        table_for_baskets[column] = table_for_baskets[column].fillna(table_for_baskets.groupby('country')[column].transform('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the numbers \n",
    "round(table_for_baskets,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the columns for baskets and drop the used columns\n",
    "\n",
    "\n",
    "table_for_baskets[\"meal_outside_for_two\"] = table_for_baskets[\"mid_range_meal_for_two\"]+ table_for_baskets[\"water_0.33_restaurants\"]+ table_for_baskets[\"domestic_beer_0.5\"]\n",
    "table_for_baskets[\"grocery_basket\"] = table_for_baskets[\"milk_1_liter\"] + table_for_baskets[\"loaf_fresh_white_bread_500g\"] + table_for_baskets[\"white_rice_1kg\"] + table_for_baskets[\"12_eggs\"] + table_for_baskets[\"local_cheese_1kg\"] + table_for_baskets[\"chicken_fillet_1kg\"] + table_for_baskets[\"apples_1kg\"] + table_for_baskets[\"tomato_1kg\"] + table_for_baskets[\"potato_1kg\"] + table_for_baskets[\"onion_1kg\"] + table_for_baskets[\"lettuce_1_head\"] + table_for_baskets[\"water_1.5ltr_market\"] + table_for_baskets[\"domestic_beer_0.5ltr_market\"]\n",
    "table_for_baskets[\"shopping\"] = table_for_baskets[\"one_jeans\"] + table_for_baskets[\"one_summer_dress\"] + table_for_baskets[\"nike_running_shoes_pair\"]\n",
    "\n",
    "table_for_baskets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_european_living_costs_baskets = table_for_baskets[[\"city\",\n",
    "                                                 \"country\",\n",
    "                                                  \"mac_meal\",\n",
    "                                                  \"meal_outside_for_two\",\n",
    "                                                  \"cappuccino_restaurants\",\n",
    "                                                  \"grocery_basket\",\n",
    "                                                  \"gasoline_1ltr\",\n",
    "                                                  \"taxi_1km\",\n",
    "                                                  \"internet\",\n",
    "                                                  \"cinema_1seat\",\n",
    "                                                  \"kindergarten_full_day_private_monthly\",\n",
    "                                                  \"shopping\"\n",
    "                                                     ]].copy()\n",
    "#final_european_living_costs_baskets.head(5)\n",
    "final_european_living_costs_baskets.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the final tableau table from Dbeaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: password, host and username are in a different notebook, which will not be uploaded to github due to gitignore\n",
    "from sql_functions  import get_dataframe\n",
    "#define schema\n",
    "schema = 'bestteamever'\n",
    "\n",
    "# Import the needed tables from sql server and put it in dataframes\n",
    "\n",
    "final_tableau_table = get_dataframe(f'select * from {schema}.tableau_final_table;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableau_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_columns_only = final_tableau_table.drop(['broadband_internet_speed', 'mobile_internet_speed', 'crime index', '1.1.2022', '1.4.2022', '1.7.2022', '1.10.2022',\n",
    "       '1.1.2022_2', '1.4.2022_2', '1.7.2022_2', '1.10.2022_2', '1.1.2022_3',\n",
    "       '1.4.2022_3', '1.7.2022_3', '1.10.2022_3'], axis = 1) #inplace= true, may be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_columns_only.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider 1 single Saturday from a month\n",
    "If you decide to spend one whole day outside you might have to have min budget of 'x'\n",
    "- you drink coffee 2 times a day  (x2)\n",
    "- you eat meal 2 times a day (x1)\n",
    "- you use taxi for 20 kms in a single day (x20)\n",
    "- you do the groceries for the rest of the week (x1)\n",
    "- and you watch a cinema in theatre (x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame.sum() to Sum of each row\n",
    "df2 = expense_columns_only.sum(axis=1)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_groceries_week_single_person = final_european_living_costs[[\"city\",\n",
    "                                                 \"country\",\n",
    "                                                 \"milk_1_liter\",\n",
    "                                                  \"loaf_fresh_white_bread_500g\",\n",
    "                                                  \"white_rice_1kg\",\n",
    "                                                  \"12_eggs\",\n",
    "                                                  \"local_cheese_1kg\",\n",
    "                                                  \"chicken_fillet_1kg\",\n",
    "                                                  \"apples_1kg\",\n",
    "                                                  \"tomato_1kg\",\n",
    "                                                  \"potato_1kg\",\n",
    "                                                  \"onion_1kg\",\n",
    "                                                  \"lettuce_1_head\",\n",
    "                                                  \"water_1.5ltr_market\",\n",
    "                                                  \"domestic_beer_0.5ltr_market\",\n",
    "                                                ]].copy()\n",
    "table_for_baskets[table_for_baskets.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_groceries_week_single_person.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_groceries_week_single_person.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_columns_only.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coffee = (expense_columns_only['cappuccino_restaurants'] * 2).to_frame('cappuccino_twice')\n",
    "print(df_coffee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_columns_only ['cappuccino_twice'] = df_coffee\n",
    "expense_columns_only.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 16:12:19) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81d278bed5b5b59425dcb5a82ce505657686243c184b4a6b67e69d01c4d432e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
